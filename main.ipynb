{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d3ec01",
   "metadata": {},
   "source": [
    "## üîß Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1340ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"‚úì Running on Google Colab\")\n",
    "    !nvidia-smi -L\n",
    "    !nvcc --version\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ddd14",
   "metadata": {},
   "source": [
    "## üì• Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2354c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone https://github.com/Ryen264/Parallel-Autoencoder-Unsupervised-Feature-Learning.git\n",
    "    %cd Parallel-Autoencoder-Unsupervised-Feature-Learning\n",
    "    print(\"\\n‚úì Repository cloned\")\n",
    "    !ls -la src/\n",
    "    !ls -la include/\n",
    "else:\n",
    "    print(\"Using local directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05263ad1",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607694a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib scikit-learn seaborn pandas -q\n",
    "if IN_COLAB:\n",
    "    !apt-get install -y libsvm-dev > /dev/null 2>&1\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932949d7",
   "metadata": {},
   "source": [
    "## üìä Download CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bceade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "if not os.path.exists(\"data/cifar-10-batches-bin\"):\n",
    "    print(\"Downloading CIFAR-10...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\",\n",
    "        \"cifar-10-binary.tar.gz\"\n",
    "    )\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    with tarfile.open(\"cifar-10-binary.tar.gz\", 'r:gz') as tar:\n",
    "        tar.extractall('data/')\n",
    "    os.remove(\"cifar-10-binary.tar.gz\")\n",
    "    print(\"‚úì Downloaded\")\n",
    "else:\n",
    "    print(\"‚úì CIFAR-10 exists\")\n",
    "\n",
    "!ls -lh data/cifar-10-batches-bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34564a5b",
   "metadata": {},
   "source": [
    "## üî® Compile CUDA Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ff700",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/main.cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <string.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <fstream>\n",
    "#include \"cpu/cpu_autoencoder.h\"\n",
    "#include \"dataset.h\"\n",
    "#include \"constants.h\"\n",
    "#include \"data_loader.h\"\n",
    "\n",
    "Dataset convertToDataset(float* images, int* labels, int n, int width, int depth) {\n",
    "    std::unique_ptr<float[]> data = std::make_unique<float[]>(n * width * width * depth);\n",
    "    std::unique_ptr<int[]> label_data = std::make_unique<int[]>(n);\n",
    "    memcpy(data.get(), images, n * width * width * depth * sizeof(float));\n",
    "    memcpy(label_data.get(), labels, n * sizeof(int));\n",
    "    return Dataset(data, label_data, n, width, depth);\n",
    "}\n",
    "\n",
    "void saveFeatures(const char* filename, Dataset& features) {\n",
    "    std::ofstream file(filename, std::ios::binary);\n",
    "    int n = features.n, width = features.width, depth = features.depth;\n",
    "    int size = n * width * width * depth;\n",
    "    file.write((char*)&n, sizeof(int));\n",
    "    file.write((char*)&width, sizeof(int));\n",
    "    file.write((char*)&depth, sizeof(int));\n",
    "    file.write((char*)features.get_data(), size * sizeof(float));\n",
    "    file.write((char*)features.get_labels(), n * sizeof(int));\n",
    "    file.close();\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"==========================================================\\n\");\n",
    "    printf(\"CIFAR-10 Autoencoder Pipeline - CUDA Implementation\\n\");\n",
    "    printf(\"==========================================================\\n\");\n",
    "    \n",
    "    const char* data_dir = \"./data/cifar-10-batches-bin\";\n",
    "    const int n_epochs = 20, batch_size = 128;\n",
    "    const float learning_rate = 0.001f;\n",
    "    const int checkpoint = 5;\n",
    "    \n",
    "    int deviceCount;\n",
    "    bool use_cuda = (cudaGetDeviceCount(&deviceCount) == cudaSuccess && deviceCount > 0);\n",
    "    if (use_cuda) {\n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, 0);\n",
    "        printf(\"\\nCUDA Device: %s\\n\", prop.name);\n",
    "    } else {\n",
    "        printf(\"\\nCPU mode\\n\");\n",
    "    }\n",
    "    \n",
    "    printf(\"\\n==========================================================\\n\");\n",
    "    printf(\"Step 1: Loading CIFAR-10\\n\");\n",
    "    printf(\"==========================================================\\n\");\n",
    "    CIFAR10Dataset* cifar_data = initCIFAR10Dataset(data_dir, use_cuda);\n",
    "    printf(\"‚úì Loaded\\n\");\n",
    "    \n",
    "    printf(\"\\n==========================================================\\n\");\n",
    "    printf(\"Step 2: Training Autoencoder\\n\");\n",
    "    printf(\"==========================================================\\n\");\n",
    "    printf(\"Epochs: %d, Batch: %d, LR: %.4f\\n\\n\", n_epochs, batch_size, learning_rate);\n",
    "    \n",
    "    Cpu_Autoencoder autoencoder;\n",
    "    Dataset train_dataset = convertToDataset(getTrainImages(cifar_data), \n",
    "                                             getTrainLabels(cifar_data), 50000, 32, 3);\n",
    "    autoencoder.fit(train_dataset, n_epochs, batch_size, learning_rate, true, checkpoint);\n",
    "    printf(\"\\n‚úì Training completed\\n\");\n",
    "    \n",
    "    printf(\"\\n==========================================================\\n\");\n",
    "    printf(\"Step 3: Extracting Features\\n\");\n",
    "    printf(\"==========================================================\\n\");\n",
    "    \n",
    "    Dataset train_features = autoencoder.encode(train_dataset);\n",
    "    printf(\"Train features: (%d,%d,%d,%d)\\n\", train_features.n, train_features.width, \n",
    "           train_features.width, train_features.depth);\n",
    "    \n",
    "    Dataset test_dataset = convertToDataset(getTestImages(cifar_data),\n",
    "                                           getTestLabels(cifar_data), 10000, 32, 3);\n",
    "    Dataset test_features = autoencoder.encode(test_dataset);\n",
    "    printf(\"Test features: (%d,%d,%d,%d)\\n\", test_features.n, test_features.width,\n",
    "           test_features.width, test_features.depth);\n",
    "    \n",
    "    printf(\"\\nSaving features...\\n\");\n",
    "    saveFeatures(\"train_features.bin\", train_features);\n",
    "    saveFeatures(\"test_features.bin\", test_features);\n",
    "    printf(\"‚úì Saved\\n\");\n",
    "    \n",
    "    freeCIFAR10Dataset(cifar_data);\n",
    "    printf(\"\\n==========================================================\\n\");\n",
    "    printf(\"C++ Pipeline Complete\\n\");\n",
    "    printf(\"==========================================================\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2475bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compiling CUDA code...\\n\")\n",
    "!nvcc -std=c++14 -O3 -I./include \\\n",
    "      src/main_colab.cu \\\n",
    "      src/data_loader.cu \\\n",
    "      src/autoencoder.cu \\\n",
    "      src/dataset.cu \\\n",
    "      src/cpu/*.cu \\\n",
    "      -o pipeline_cuda\n",
    "\n",
    "if os.path.exists('pipeline_cuda'):\n",
    "    print(\"\\n‚úì Compilation successful!\")\n",
    "    !ls -lh pipeline_cuda\n",
    "else:\n",
    "    print(\"\\n‚úó Compilation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31452b3f",
   "metadata": {},
   "source": [
    "## üöÄ Run CUDA Pipeline (Steps 1-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11896c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running CUDA pipeline (15-30 minutes)...\\n\")\n",
    "!./pipeline_cuda\n",
    "\n",
    "print(\"\\nChecking output files:\")\n",
    "if os.path.exists('train_features.bin') and os.path.exists('test_features.bin'):\n",
    "    !ls -lh train_features.bin test_features.bin\n",
    "    print(\"‚úì C++ pipeline completed!\")\n",
    "else:\n",
    "    print(\"‚úó Feature files not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a906d0b",
   "metadata": {},
   "source": [
    "## üì• Load Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "def load_features(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        n = struct.unpack('i', f.read(4))[0]\n",
    "        width = struct.unpack('i', f.read(4))[0]\n",
    "        depth = struct.unpack('i', f.read(4))[0]\n",
    "        feature_size = n * width * width * depth\n",
    "        features = np.frombuffer(f.read(feature_size * 4), dtype=np.float32)\n",
    "        features = features.reshape(n, width, width, depth)\n",
    "        labels = np.frombuffer(f.read(n * 4), dtype=np.int32)\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = load_features('train_features.bin')\n",
    "test_features, test_labels = load_features('test_features.bin')\n",
    "\n",
    "print(f\"‚úì Train features: {train_features.shape}\")\n",
    "print(f\"‚úì Test features: {test_features.shape}\")\n",
    "\n",
    "train_features_flat = train_features.reshape(train_features.shape[0], -1)\n",
    "test_features_flat = test_features.reshape(test_features.shape[0], -1)\n",
    "\n",
    "print(f\"\\n‚úì Flattened train: {train_features_flat.shape}\")\n",
    "print(f\"‚úì Flattened test: {test_features_flat.shape}\")\n",
    "print(f\"‚úì Feature dimension: {train_features_flat.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445d1a2",
   "metadata": {},
   "source": [
    "## üéì Step 4: Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Step 4: Training SVM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Kernel: RBF, C: 10.0, Gamma: auto\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_features_flat)\n",
    "test_scaled = scaler.transform(test_features_flat)\n",
    "\n",
    "svm = SVC(kernel='rbf', C=10.0, gamma='auto', verbose=True, cache_size=1000)\n",
    "\n",
    "print(\"Training SVM (10-20 minutes)...\")\n",
    "start = time.time()\n",
    "svm.fit(train_scaled, train_labels)\n",
    "duration = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Trained in {duration:.2f}s ({duration/60:.2f} min)\")\n",
    "print(f\"‚úì Support vectors: {svm.n_support_.sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac39cc",
   "metadata": {},
   "source": [
    "## üìä Step 5: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "y_pred = svm.predict(test_scaled)\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "\n",
    "print(f\"\\n‚úì Test Accuracy: {accuracy*100:.2f}%\\n\")\n",
    "print(classification_report(test_labels, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - CUDA Autoencoder + SVM', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(class_names, class_acc * 100)\n",
    "plt.axhline(y=accuracy*100, color='r', linestyle='--', linewidth=2,\n",
    "            label=f'Overall: {accuracy*100:.2f}%')\n",
    "plt.title('Per-Class Accuracy', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, acc in zip(bars, class_acc):\n",
    "    bar.set_color('green' if acc >= accuracy else 'orange')\n",
    "    bar.set_alpha(0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-Class Results:\")\n",
    "for name, acc in zip(class_names, class_acc):\n",
    "    print(f\"{name:12s}: {acc*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81d166",
   "metadata": {},
   "source": [
    "## üìù Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ec92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CUDA AUTOENCODER + SVM PIPELINE - FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüîß Implementation:\")\n",
    "print(\"  ‚Ä¢ Data: data_loader.cu (CUDA)\")\n",
    "print(\"  ‚Ä¢ Autoencoder: autoencoder.cu + cpu_autoencoder.cu\")\n",
    "print(\"  ‚Ä¢ SVM: scikit-learn (Python)\")\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "print(f\"  ‚Ä¢ Training samples: 50,000\")\n",
    "print(f\"  ‚Ä¢ Test samples: 10,000\")\n",
    "print(f\"  ‚Ä¢ Feature dimension: {train_features_flat.shape[1]:,}\")\n",
    "print(f\"  ‚Ä¢ Compression: {(32*32*3) / train_features_flat.shape[1]:.2f}x\")\n",
    "print(f\"  ‚Ä¢ Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Best class: {class_names[np.argmax(class_acc)]} ({class_acc.max()*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Worst class: {class_names[np.argmin(class_acc)]} ({class_acc.min()*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline completed! C++ (CUDA) ‚Üí Python (SVM) working!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import json\n",
    "with open('cuda_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'accuracy': float(accuracy),\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'class_accuracy': class_acc.tolist(),\n",
    "        'implementation': 'CUDA C++ + Python'\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\nüíæ Results saved to cuda_results.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
